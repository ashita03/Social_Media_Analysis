---
title: "Logistic Regression on Social Media Data"
output: html_document
---

```{r}
library(ggplot2)
library(cowplot)
#library(regclass)
library(caret)

library(e1071)
library(pROC)
```


### **Predicting the Impact on Mood Productivity based on Social Media Usage**

```{r}
social_media <- read.csv("C:\\Users\\Dell\\Desktop\\RBS\\Semester II\\Multivariate Analysis\\Assignment\\Social_Media\\MVA_Social_Media_Cleaned.csv", row.names = 1)

```

* We can convert columns "Mood_Productivity", "Tired_Morning", "Trouble_Sleeping", and "Entire_Week_Feeling" into factors.

**Converting necessary columns into Factors**

```{r}

social_media$Mood_Productivity <- ifelse(test=social_media$Mood_Productivity == 'Yes', yes="Productive", no="Not Productive") 
social_media$Mood_Productivity <- as.factor(social_media$Mood_Productivity)

#social_media$Mood_Productivity <- as.factor(social_media$Mood_Productivity) 
social_media$Tired_Morning <- as.factor(social_media$Tired_Morning) 
social_media$Trouble_Sleeping <- as.factor(social_media$Trouble_Sleeping) 
```

```{r}
str(social_media)
```

```{r}
social_media_reg <- social_media[, c("Instagram_Hours",	"LinkedIn_Hours",	"Snapchat_Hours", "Twitter_Hours",	"Whatsapp_Wechat_hours", "Reddit_hours",	"Youtube_hours",	"OTT_hours", "Mood_Productivity", "Tired_Morning", "Trouble_Sleeping")]
```

#### **Model Development**

```{r}
logistic_reg <- glm(Mood_Productivity ~Instagram_Hours+LinkedIn_Hours+Snapchat_Hours+Twitter_Hours+Whatsapp_Wechat_hours+Reddit_hours+Youtube_hours+OTT_hours+Tired_Morning+Trouble_Sleeping, data=social_media_reg, family="binomial")

summary(logistic_reg)
```

**Inference:**

* None of the predictor variables ("Instagram_Hours", "LinkedIn_Hours", "Snapchat_Hours", etc.) have coefficients with statistically significant p-values (all p-values are 1). This suggests that there is insufficient evidence to conclude that any of these variables have a significant impact on mood productivity.


#### Now calculate the overall "Pseudo R-squared" and its p-value

```{r}
ll.null <- logistic_reg$null.deviance/-2
ll.proposed <- logistic_reg$deviance/-2
(ll.null - ll.proposed) / ll.null
```

**Inference:**

* The Pseudo R-squared measures the proportion of total variance in the response variable that is explained by the model.
* The output of 1 indicates that the model fits the data perfectly. However, its very rare when the model could fit perfectly, signifying an *overfitting model*.

#### The p-value for the R^2

```{r}
1 - pchisq(2*(ll.proposed - ll.null), df=(length(logistic_reg$coefficients)-1))
```

**Inference:**

* As the p-value for R^2 is greater than 0.05, we fail to reject the null hypothesis which is the model is better than the null model (without predictors).


#### **Residual Analysis**

```{r}
residuals <- residuals(logistic_reg, type = "response")
```

```{r}
plot(logistic_reg)
```

**Inference:**

* Residuals vs Fitted are a great way to identify heteroscedasticity via pattern recognition, however our model does not seem to have any.
* Q-Q Plots are great in understanding the distribution of residuals. As it can be observed that there is quite some deviation from the straight line indicating that residuals do not follow a normal distribution.
* Scale-Location Plot shows the square root of the standardized residuals against the fitted values. As it may be observed that the spread of points is not consistent across the range of fitted values, it indicates that the variability of the residuals do not remain constant.
* Residuals Vs Leverage help in identifying potential problems with the regression model. Points on the right of the plot have a great influence on the model's parameters. Points with a Cook's Distance greater than highlighted threshold have a greater influence on the model.


#### **Predicted Data**

```{r}
predicted.data <- data.frame(probability.of.mood=logistic_reg$fitted.values,mood=social_media_reg$Mood_Productivity)
predicted.data <- predicted.data[order(predicted.data$probability.of.mood, decreasing=FALSE),]
predicted.data$rank <- 1:nrow(predicted.data)

predicted.data
```

**Inference:**

* The dataframe provides with the probability of the moods being "Yes" or "No".
* From the table, it can be inferred that most of the observations have a higher probability of being "Yes"


#### **Plotting of predicted probabilities**

```{r}
ggplot(data=predicted.data, aes(x=rank, y=probability.of.mood)) +
geom_point(aes(color=mood), alpha=1, shape=4, stroke=2) +
xlab("Index") +
ylab("Predicted probability of Mood")
```

**Inference:**

* This plot highlights the performace of our model.
* It can be clearly inferred that the model performs poorly.

#### **Model Evaluation (Accuracy)**

```{r}
# From Caret
pdata <- predict(logistic_reg,newdata=social_media_reg,type="response" )
pdata
```

**Inference:**

* Each value in the pdata object represents the predicted probability of mood being "Yes" for the corresponding observation in your dataset.

```{r}
pdataF <- as.factor(ifelse(test=as.numeric(pdata>0.5) == 0, yes="Not Productive", no="Productive"))

#From e1071::
confusionMatrix(pdataF, social_media_reg$Mood_Productivity)
```

**Inference:**
*Positive Class - Not Productive* - Since we are aiming to see how many students felt unproductive, based on their social media usage.

* The accuracy of the model is 1 showcasing that it is a well performing model. However, such a high accuracy is a clear indication of model overfitting.
* 95% Confidence interval gives the confidence intervals for accuracy ranges which is between 0.8389 and 1. This indicates that we can be confidence that the accuracy of the model is at least 83.89%
* The proportion for the majority class is 0.95.
* Sensitivity is the measure of proportion of actual positive classes that were correctly identified by the model, which in our case is 1.
* Specificity is the measure of proportion of actual negative classes that were correctly identified by the model, which in our case is 1.


#### **SUMMARY**

* We can infer that due to the small dataset, it was difficult to fit the model well which led to an under-performing model.
* Another aspect that calls to attention the bias present in the data is how majority of our logged data points had a 'Yes' for Mood Productivity with just one 'No' for one of the data points.
* Therefore, despite aiming to understand the mood productivity of a student based on their social media usage, it is difficult due to the above mentioned reasons.
